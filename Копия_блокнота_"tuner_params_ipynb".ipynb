{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"tuner_params.ipynb\"",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyODXkXjwbnwIEu475b4ERdX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaintKyou/ML/blob/master/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22tuner_params_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9d-uWoR-9uf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "8d0d8370-1742-49d2-a585-65c85821ac1b"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('output.csv')\n",
        "data[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>s6</th>\n",
              "      <th>s7</th>\n",
              "      <th>s8</th>\n",
              "      <th>s9</th>\n",
              "      <th>s10</th>\n",
              "      <th>s11</th>\n",
              "      <th>s12</th>\n",
              "      <th>s13</th>\n",
              "      <th>s14</th>\n",
              "      <th>s15</th>\n",
              "      <th>s16</th>\n",
              "      <th>s17</th>\n",
              "      <th>s18</th>\n",
              "      <th>s19</th>\n",
              "      <th>s20</th>\n",
              "      <th>s21</th>\n",
              "      <th>s22</th>\n",
              "      <th>s23</th>\n",
              "      <th>s24</th>\n",
              "      <th>s25</th>\n",
              "      <th>s26</th>\n",
              "      <th>s27</th>\n",
              "      <th>s28</th>\n",
              "      <th>s29</th>\n",
              "      <th>s30</th>\n",
              "      <th>s31</th>\n",
              "      <th>s32</th>\n",
              "      <th>s33</th>\n",
              "      <th>s34</th>\n",
              "      <th>s35</th>\n",
              "      <th>s36</th>\n",
              "      <th>tet</th>\n",
              "      <th>phi</th>\n",
              "      <th>kr</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>Ne</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>37.48510</td>\n",
              "      <td>21.47650</td>\n",
              "      <td>72.03370</td>\n",
              "      <td>74.87260</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>7.15842</td>\n",
              "      <td>2.86372</td>\n",
              "      <td>9.73767</td>\n",
              "      <td>3.95813</td>\n",
              "      <td>6.82865</td>\n",
              "      <td>20.3750</td>\n",
              "      <td>7.36746</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>10.39340</td>\n",
              "      <td>7.18657</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2.69125</td>\n",
              "      <td>2.48769</td>\n",
              "      <td>4.6243</td>\n",
              "      <td>18.67210</td>\n",
              "      <td>1.56277</td>\n",
              "      <td>6.99705</td>\n",
              "      <td>2.01758</td>\n",
              "      <td>4.62862</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>9.33522</td>\n",
              "      <td>1.61079</td>\n",
              "      <td>4.89950</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>5.03119</td>\n",
              "      <td>19.24120</td>\n",
              "      <td>18.36110</td>\n",
              "      <td>6.50392</td>\n",
              "      <td>9.45598</td>\n",
              "      <td>8.83209</td>\n",
              "      <td>222.3430</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-33.7909</td>\n",
              "      <td>-10.0577</td>\n",
              "      <td>33905.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.08870</td>\n",
              "      <td>7.55648</td>\n",
              "      <td>28.97700</td>\n",
              "      <td>44.00660</td>\n",
              "      <td>2.01309</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>8.88389</td>\n",
              "      <td>4.70197</td>\n",
              "      <td>4.70138</td>\n",
              "      <td>9.87103</td>\n",
              "      <td>11.0701</td>\n",
              "      <td>4.42813</td>\n",
              "      <td>11.1692</td>\n",
              "      <td>5.97066</td>\n",
              "      <td>5.42396</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2.03481</td>\n",
              "      <td>2.33048</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.63118</td>\n",
              "      <td>29.05450</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>25.82190</td>\n",
              "      <td>13.31970</td>\n",
              "      <td>5.86606</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>40.72400</td>\n",
              "      <td>155.3300</td>\n",
              "      <td>7.0</td>\n",
              "      <td>-33.7909</td>\n",
              "      <td>-10.0577</td>\n",
              "      <td>22573.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32.82920</td>\n",
              "      <td>144.71500</td>\n",
              "      <td>246.86400</td>\n",
              "      <td>83.69460</td>\n",
              "      <td>10.50480</td>\n",
              "      <td>28.81570</td>\n",
              "      <td>28.77490</td>\n",
              "      <td>36.16710</td>\n",
              "      <td>7.96149</td>\n",
              "      <td>24.13200</td>\n",
              "      <td>22.7822</td>\n",
              "      <td>7.48365</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>7.35025</td>\n",
              "      <td>2.71817</td>\n",
              "      <td>5.37989</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.72421</td>\n",
              "      <td>2.46114</td>\n",
              "      <td>4.67105</td>\n",
              "      <td>20.8686</td>\n",
              "      <td>6.82867</td>\n",
              "      <td>3.77993</td>\n",
              "      <td>4.43100</td>\n",
              "      <td>2.79168</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2.60397</td>\n",
              "      <td>4.06729</td>\n",
              "      <td>3.08867</td>\n",
              "      <td>2.98717</td>\n",
              "      <td>2.83806</td>\n",
              "      <td>2.08980</td>\n",
              "      <td>56.50520</td>\n",
              "      <td>34.54030</td>\n",
              "      <td>27.97570</td>\n",
              "      <td>32.25710</td>\n",
              "      <td>45.21980</td>\n",
              "      <td>301.3900</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-33.7909</td>\n",
              "      <td>-10.0577</td>\n",
              "      <td>65195.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.48722</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2.09672</td>\n",
              "      <td>9.09409</td>\n",
              "      <td>2.44893</td>\n",
              "      <td>8.25071</td>\n",
              "      <td>10.60860</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>9.06438</td>\n",
              "      <td>8.90096</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>48.65310</td>\n",
              "      <td>74.7579</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-33.7909</td>\n",
              "      <td>-10.0577</td>\n",
              "      <td>5262.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12.42410</td>\n",
              "      <td>7.65120</td>\n",
              "      <td>11.86360</td>\n",
              "      <td>38.41840</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.71730</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.54347</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>9.78031</td>\n",
              "      <td>8.81661</td>\n",
              "      <td>1.78242</td>\n",
              "      <td>3.93061</td>\n",
              "      <td>43.13070</td>\n",
              "      <td>187.1960</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-33.7909</td>\n",
              "      <td>-10.0577</td>\n",
              "      <td>11204.50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         s1         s2         s3        s4  ...   kr        X        Y        Ne\n",
              "0  37.48510   21.47650   72.03370  74.87260  ...  9.0 -33.7909 -10.0577  33905.80\n",
              "1  20.08870    7.55648   28.97700  44.00660  ...  7.0 -33.7909 -10.0577  22573.60\n",
              "2  32.82920  144.71500  246.86400  83.69460  ...  9.0 -33.7909 -10.0577  65195.60\n",
              "3   6.48722    0.00000    2.09672   9.09409  ...  3.0 -33.7909 -10.0577   5262.08\n",
              "4  12.42410    7.65120   11.86360  38.41840  ...  3.0 -33.7909 -10.0577  11204.50\n",
              "\n",
              "[5 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTH36-tjF9zF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "985b9359-9128-41f9-9cb7-b4eadd59e222"
      },
      "source": [
        "pip install -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (20.7)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner) (0.17.0)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp36-none-any.whl size=78939 sha256=b92c0d24be78dd4ab8aba1100899cd4eb8cf5b4ac123b6d34e54a6b56142b111\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15357 sha256=2a80c00bc6a8a2dff364e10386af031dc505bce7c6b5531587196373a9cc7fd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbdpi3VdHwyS"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras import regularizers\n",
        "from google.colab import files\n",
        "from kerastuner.tuners import BayesianOptimization\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1zT_-OKY_61"
      },
      "source": [
        "df = data[data.kr>6]\n",
        "df = df[df.X>=-37.55] # отбираем события внутри установки\n",
        "df = df[df.X<=42.13]\n",
        "df = df[df.Y>=-70.50]\n",
        "df = df[df.Y<=64.66]\n",
        "df = df.drop(columns=[\"kr\"])\n",
        "df = df.drop(columns=[\"Ne\"])\n",
        "df = df.drop(columns=[\"X\"])\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = df[\"Y\"]\n",
        "X = df.drop(columns=[\"Y\"])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFEXf1QnZI6J"
      },
      "source": [
        "# Среднее значение\n",
        "mean = X_train.mean(axis=0)\n",
        "# Стандартное отклонение\n",
        "std = X_train.std(axis=0)\n",
        "X_train -= mean\n",
        "X_train /= std\n",
        "X_test -= mean\n",
        "X_test /= std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GWhYx8rZMaG"
      },
      "source": [
        "from tensorflow.keras import initializers\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    for i in range(hp.Int('num_layers', 2, 20)):  # кол-во слоев (мин, макс)\n",
        "        model.add(Dense(units=hp.Int('units_' + str(i),\n",
        "                                            min_value=38,\n",
        "                                            max_value=512,\n",
        "                                            step=32),\n",
        "                               activation='relu', activity_regularizer=regularizers.l2(1e-4), kernel_initializer='he_uniform')) \n",
        "    model.add(Dense(1,kernel_initializer='glorot_uniform')) # число нейронов на выходном слое\n",
        "    model.compile(\n",
        "        optimizer=hp.Choice('optimizer', values=['adam']),\n",
        "        loss='mse',\n",
        "        metrics=['mae'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LhtK8UObpRO"
      },
      "source": [
        "tuner = BayesianOptimization(\n",
        "    build_model,                 # функция создания модели\n",
        "    objective='mae',             # метрика, которую нужно оптимизировать - \n",
        "                                 \n",
        "    max_trials=80,               # максимальное количество запусков обучения \n",
        "    directory='abcd'   # каталог, куда сохраняются обученные сети  \n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1B5o3ync2Ub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f490bca5-021d-4bf7-e69b-2e325cb75b85"
      },
      "source": [
        "tuner.search_space_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 4\n",
            "num_layers (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 20, 'step': 1, 'sampling': None}\n",
            "units_0 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 38, 'max_value': 512, 'step': 32, 'sampling': None}\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 38, 'max_value': 512, 'step': 32, 'sampling': None}\n",
            "optimizer (Choice)\n",
            "{'default': 'adam', 'conditions': [], 'values': ['adam'], 'ordered': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKx2XQNJc8Tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97a9c861-58d1-4dc2-8026-1ec1605d9e62"
      },
      "source": [
        "tuner.search(X_train,                  # Данные для обучения\n",
        "             y_train,                  # Правильные ответы\n",
        "             batch_size=256,           # Размер мини-выборки\n",
        "             epochs=20,                # Количество эпох обучения \n",
        "             validation_split=0.2,     # Часть данных, которая будет использоваться для проверки\n",
        "             )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 80 Complete [00h 00m 55s]\n",
            "mae: 4.016036033630371\n",
            "\n",
            "Best mae So Far: 3.5370845794677734\n",
            "Total elapsed time: 01h 20m 00s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbgtQTB0l4nW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52eaa2fd-d610-4b29-e7c7-48a58fc95e90"
      },
      "source": [
        "tuner.results_summary() # результаты"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in abcd/untitled_project\n",
            "Showing 10 best trials\n",
            "Objective(name='mae', direction='min')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 20\n",
            "units_0: 486\n",
            "units_1: 486\n",
            "optimizer: adam\n",
            "units_2: 486\n",
            "units_3: 486\n",
            "units_4: 486\n",
            "units_5: 486\n",
            "units_6: 38\n",
            "units_7: 486\n",
            "units_8: 38\n",
            "units_9: 38\n",
            "units_10: 38\n",
            "units_11: 486\n",
            "units_12: 38\n",
            "units_13: 486\n",
            "units_14: 38\n",
            "units_15: 486\n",
            "units_16: 38\n",
            "units_17: 38\n",
            "units_18: 486\n",
            "units_19: 486\n",
            "Score: 3.5370845794677734\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 20\n",
            "units_0: 486\n",
            "units_1: 486\n",
            "optimizer: adam\n",
            "units_2: 486\n",
            "units_3: 486\n",
            "units_4: 38\n",
            "units_5: 38\n",
            "units_6: 38\n",
            "units_7: 38\n",
            "units_8: 486\n",
            "units_9: 38\n",
            "units_10: 38\n",
            "units_11: 38\n",
            "units_12: 486\n",
            "units_13: 38\n",
            "units_14: 38\n",
            "units_15: 38\n",
            "units_16: 486\n",
            "units_17: 38\n",
            "units_18: 486\n",
            "units_19: 486\n",
            "Score: 3.606062412261963\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 20\n",
            "units_0: 486\n",
            "units_1: 486\n",
            "optimizer: adam\n",
            "units_2: 486\n",
            "units_3: 486\n",
            "units_4: 486\n",
            "units_5: 486\n",
            "units_6: 486\n",
            "units_7: 38\n",
            "units_8: 38\n",
            "units_9: 38\n",
            "units_10: 38\n",
            "units_11: 38\n",
            "units_12: 38\n",
            "units_13: 38\n",
            "units_14: 38\n",
            "units_15: 486\n",
            "units_16: 38\n",
            "units_17: 38\n",
            "units_18: 486\n",
            "units_19: 486\n",
            "Score: 3.6580755710601807\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 20\n",
            "units_0: 486\n",
            "units_1: 486\n",
            "optimizer: adam\n",
            "units_2: 486\n",
            "units_3: 486\n",
            "units_4: 486\n",
            "units_5: 486\n",
            "units_6: 486\n",
            "units_7: 486\n",
            "units_8: 486\n",
            "units_9: 38\n",
            "units_10: 38\n",
            "units_11: 38\n",
            "units_12: 486\n",
            "units_13: 486\n",
            "units_14: 38\n",
            "units_15: 38\n",
            "units_16: 38\n",
            "units_17: 38\n",
            "units_18: 486\n",
            "units_19: 486\n",
            "Score: 3.6620192527770996\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 20\n",
            "units_0: 486\n",
            "units_1: 486\n",
            "optimizer: adam\n",
            "units_2: 486\n",
            "units_3: 486\n",
            "units_4: 486\n",
            "units_5: 486\n",
            "units_6: 38\n",
            "units_7: 486\n",
            "units_8: 38\n",
            "units_9: 38\n",
            "units_10: 38\n",
            "units_11: 38\n",
            "units_12: 38\n",
            "units_13: 486\n",
            "units_14: 38\n",
            "units_15: 38\n",
            "units_16: 38\n",
            "units_17: 38\n",
            "units_18: 486\n",
            "units_19: 38\n",
            "Score: 3.6730165481567383\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 20\n",
            "units_0: 486\n",
            "units_1: 486\n",
            "optimizer: adam\n",
            "units_2: 486\n",
            "units_3: 486\n",
            "units_4: 486\n",
            "units_5: 486\n",
            "units_6: 38\n",
            "units_7: 38\n",
            "units_8: 486\n",
            "units_9: 38\n",
            "units_10: 38\n",
            "units_11: 486\n",
            "units_12: 38\n",
            "units_13: 38\n",
            "units_14: 38\n",
            "units_15: 486\n",
            "units_16: 486\n",
            "units_17: 38\n",
            "units_18: 486\n",
            "units_19: 38\n",
            "Score: 3.698944330215454\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 20\n",
            "units_0: 486\n",
            "units_1: 486\n",
            "optimizer: adam\n",
            "units_2: 486\n",
            "units_3: 486\n",
            "units_4: 38\n",
            "units_5: 486\n",
            "units_6: 38\n",
            "units_7: 486\n",
            "units_8: 38\n",
            "units_9: 38\n",
            "units_10: 38\n",
            "units_11: 486\n",
            "units_12: 38\n",
            "units_13: 486\n",
            "units_14: 486\n",
            "units_15: 38\n",
            "units_16: 38\n",
            "units_17: 38\n",
            "units_18: 486\n",
            "units_19: 486\n",
            "Score: 3.705749273300171\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 20\n",
            "units_0: 486\n",
            "units_1: 38\n",
            "optimizer: adam\n",
            "units_2: 486\n",
            "units_3: 486\n",
            "units_4: 486\n",
            "units_5: 486\n",
            "units_6: 486\n",
            "units_7: 38\n",
            "units_8: 486\n",
            "units_9: 38\n",
            "units_10: 486\n",
            "units_11: 486\n",
            "units_12: 38\n",
            "units_13: 38\n",
            "units_14: 486\n",
            "units_15: 486\n",
            "units_16: 486\n",
            "units_17: 38\n",
            "units_18: 486\n",
            "units_19: 486\n",
            "Score: 3.706409215927124\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 20\n",
            "units_0: 486\n",
            "units_1: 486\n",
            "optimizer: adam\n",
            "units_2: 486\n",
            "units_3: 486\n",
            "units_4: 486\n",
            "units_5: 486\n",
            "units_6: 486\n",
            "units_7: 38\n",
            "units_8: 486\n",
            "units_9: 38\n",
            "units_10: 38\n",
            "units_11: 38\n",
            "units_12: 486\n",
            "units_13: 486\n",
            "units_14: 38\n",
            "units_15: 38\n",
            "units_16: 486\n",
            "units_17: 486\n",
            "units_18: 486\n",
            "units_19: 38\n",
            "Score: 3.7200582027435303\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 20\n",
            "units_0: 486\n",
            "units_1: 486\n",
            "optimizer: adam\n",
            "units_2: 486\n",
            "units_3: 486\n",
            "units_4: 38\n",
            "units_5: 38\n",
            "units_6: 486\n",
            "units_7: 486\n",
            "units_8: 38\n",
            "units_9: 38\n",
            "units_10: 486\n",
            "units_11: 38\n",
            "units_12: 486\n",
            "units_13: 38\n",
            "units_14: 486\n",
            "units_15: 38\n",
            "units_16: 38\n",
            "units_17: 486\n",
            "units_18: 486\n",
            "units_19: 38\n",
            "Score: 3.7512550354003906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNV6Q0vW2zj-"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0] # лучшая сеть"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHMSwYHW3EGs"
      },
      "source": [
        "mdl = tuner.hypermodel.build(best_hps) # создаем сеть с лучшими гиперпараметрами"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7-pBJmV3VYl",
        "outputId": "ca8753ff-a4f4-4596-c944-958dc19792e9"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "mdl.fit(X_train, y_train, epochs=50, batch_size=128, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "99/99 - 4s - loss: 1064.4952 - mae: 25.3405\n",
            "Epoch 2/50\n",
            "99/99 - 4s - loss: 214.0102 - mae: 10.6341\n",
            "Epoch 3/50\n",
            "99/99 - 4s - loss: 216.5816 - mae: 10.1863\n",
            "Epoch 4/50\n",
            "99/99 - 4s - loss: 122.5763 - mae: 7.5310\n",
            "Epoch 5/50\n",
            "99/99 - 4s - loss: 92.3801 - mae: 6.4846\n",
            "Epoch 6/50\n",
            "99/99 - 4s - loss: 68.4949 - mae: 5.6034\n",
            "Epoch 7/50\n",
            "99/99 - 4s - loss: 53.9331 - mae: 5.0385\n",
            "Epoch 8/50\n",
            "99/99 - 4s - loss: 62.0482 - mae: 5.4142\n",
            "Epoch 9/50\n",
            "99/99 - 4s - loss: 46.6390 - mae: 4.6750\n",
            "Epoch 10/50\n",
            "99/99 - 4s - loss: 44.3311 - mae: 4.5660\n",
            "Epoch 11/50\n",
            "99/99 - 4s - loss: 37.4015 - mae: 4.1492\n",
            "Epoch 12/50\n",
            "99/99 - 4s - loss: 44.0769 - mae: 4.3813\n",
            "Epoch 13/50\n",
            "99/99 - 4s - loss: 40.2110 - mae: 4.2395\n",
            "Epoch 14/50\n",
            "99/99 - 4s - loss: 35.7852 - mae: 4.0776\n",
            "Epoch 15/50\n",
            "99/99 - 4s - loss: 36.4654 - mae: 4.1240\n",
            "Epoch 16/50\n",
            "99/99 - 4s - loss: 38.9069 - mae: 4.2916\n",
            "Epoch 17/50\n",
            "99/99 - 4s - loss: 48.5131 - mae: 4.6199\n",
            "Epoch 18/50\n",
            "99/99 - 4s - loss: 33.2413 - mae: 3.9006\n",
            "Epoch 19/50\n",
            "99/99 - 4s - loss: 27.2020 - mae: 3.5079\n",
            "Epoch 20/50\n",
            "99/99 - 4s - loss: 51.7002 - mae: 4.6918\n",
            "Epoch 21/50\n",
            "99/99 - 4s - loss: 28.2305 - mae: 3.6231\n",
            "Epoch 22/50\n",
            "99/99 - 4s - loss: 30.9548 - mae: 3.7474\n",
            "Epoch 23/50\n",
            "99/99 - 4s - loss: 29.1519 - mae: 3.6418\n",
            "Epoch 24/50\n",
            "99/99 - 4s - loss: 31.4782 - mae: 3.6733\n",
            "Epoch 25/50\n",
            "99/99 - 4s - loss: 28.1658 - mae: 3.5483\n",
            "Epoch 26/50\n",
            "99/99 - 4s - loss: 51.3510 - mae: 4.7396\n",
            "Epoch 27/50\n",
            "99/99 - 4s - loss: 34.6629 - mae: 3.8968\n",
            "Epoch 28/50\n",
            "99/99 - 4s - loss: 40.0170 - mae: 4.3577\n",
            "Epoch 29/50\n",
            "99/99 - 4s - loss: 23.7883 - mae: 3.3145\n",
            "Epoch 30/50\n",
            "99/99 - 4s - loss: 29.3398 - mae: 3.6681\n",
            "Epoch 31/50\n",
            "99/99 - 4s - loss: 25.4869 - mae: 3.4047\n",
            "Epoch 32/50\n",
            "99/99 - 4s - loss: 21.3853 - mae: 3.1175\n",
            "Epoch 33/50\n",
            "99/99 - 4s - loss: 30.2827 - mae: 3.7431\n",
            "Epoch 34/50\n",
            "99/99 - 4s - loss: 20.7648 - mae: 3.0820\n",
            "Epoch 35/50\n",
            "99/99 - 4s - loss: 22.8914 - mae: 3.2002\n",
            "Epoch 36/50\n",
            "99/99 - 4s - loss: 22.5332 - mae: 3.2420\n",
            "Epoch 37/50\n",
            "99/99 - 4s - loss: 19.3046 - mae: 2.9619\n",
            "Epoch 38/50\n",
            "99/99 - 4s - loss: 29.9774 - mae: 3.6525\n",
            "Epoch 39/50\n",
            "99/99 - 4s - loss: 25.6564 - mae: 3.4794\n",
            "Epoch 40/50\n",
            "99/99 - 4s - loss: 20.8534 - mae: 3.0518\n",
            "Epoch 41/50\n",
            "99/99 - 4s - loss: 22.8776 - mae: 3.3193\n",
            "Epoch 42/50\n",
            "99/99 - 4s - loss: 82.2930 - mae: 6.2278\n",
            "Epoch 43/50\n",
            "99/99 - 4s - loss: 36.0005 - mae: 4.0629\n",
            "Epoch 44/50\n",
            "99/99 - 4s - loss: 26.6341 - mae: 3.5356\n",
            "Epoch 45/50\n",
            "99/99 - 4s - loss: 23.2836 - mae: 3.3099\n",
            "Epoch 46/50\n",
            "99/99 - 4s - loss: 49.5027 - mae: 4.6048\n",
            "Epoch 47/50\n",
            "99/99 - 4s - loss: 40.2617 - mae: 4.2689\n",
            "Epoch 48/50\n",
            "99/99 - 4s - loss: 31.3470 - mae: 3.7776\n",
            "Epoch 49/50\n",
            "99/99 - 4s - loss: 19.6032 - mae: 3.0690\n",
            "Epoch 50/50\n",
            "99/99 - 4s - loss: 18.8268 - mae: 2.9736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6b57b318d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoM2cz2L38ED"
      },
      "source": [
        "mdl.save(\"y_po.h5\") # сохраняем сеть"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}